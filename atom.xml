<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Qiangqiang</title>
  
  <subtitle>A graduate student who likes life and technologies</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-01-10T19:45:09.167Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Qiangqiang Guo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>4WD Car SLAM and Control (2) Initial-testing</title>
    <link href="http://example.com/2021/01/10/4WD-Car-SLAM-and-Control-2-Initial-testing/"/>
    <id>http://example.com/2021/01/10/4WD-Car-SLAM-and-Control-2-Initial-testing/</id>
    <published>2021-01-10T19:35:06.000Z</published>
    <updated>2021-01-10T19:45:09.167Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Communication-between-Mac-Raspberry-Pi-and-on-board-sensors-actuators">1. Communication between Mac, Raspberry Pi, and on-board sensors/actuators</h1><h2 id="Mac-and-Raspberry-Pi">1.1. Mac and Raspberry Pi</h2><p>The communication between Mac and Raspberry Pi can be achieved by the ssh and VNC methods introduced before.</p><h2 id="Raspberry-Pi-and-on-board-sensors-actuators">1.2. Raspberry Pi and on-board sensors/actuators</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Communication-between-Mac-Raspberry-Pi-and-on-board-sensors-actuators&quot;&gt;1. Communication between Mac, Raspberry Pi, and on-board sens</summary>
      
    
    
    
    <category term="Robotics" scheme="http://example.com/categories/Robotics/"/>
    
    
    <category term="4WD car" scheme="http://example.com/tags/4WD-car/"/>
    
    <category term="control" scheme="http://example.com/tags/control/"/>
    
    <category term="slam" scheme="http://example.com/tags/slam/"/>
    
  </entry>
  
  <entry>
    <title>A summary of RL algorithms (1) Deep Q-Learning</title>
    <link href="http://example.com/2021/01/04/A-summary-of-RL-algorithms-1-Deep-Q-Learning/"/>
    <id>http://example.com/2021/01/04/A-summary-of-RL-algorithms-1-Deep-Q-Learning/</id>
    <published>2021-01-04T20:58:12.000Z</published>
    <updated>2021-01-10T19:44:46.731Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction">1. Introduction</h1><p>In this series of blogs, I will summarize and implement some key algorithms in RL. This serves as a self-learning notes on my way to RL.</p><h1 id="Deep-Q-Learning">2. Deep Q-Learning</h1><p>This section follows the flow of this paper: <a href="https://arxiv.org/pdf/1710.02298.pdf">Rainbow: Combining Improvements in Deep Reinforcement Learning (Hessel et al., 2017)</a>, which gives a good summary of Deep Q-learning algorithms as well as test different combinations of those algorithms.</p><h2 id="Original-deep-Q-Networks-DQN">2.1. Original deep Q-Networks (DQN)</h2><p>Let’s start from the very beginning. Please refer to <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Sutton and Barto</a> for details of the definations of a Markov decision process (MDP). The notations used here are same as those used in this book. Define the state-value function for policy $\pi$</p><script type="math/tex; mode=display">v_{\pi}(s) \doteq \mathrm{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]=\mathbf{E}_{\pi}\left[\sum^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right], \text { for all } s \in \mathcal{S} \tag{1} \label{1}</script><!-- reference $\eqref{1}$ --><p>and the action-value function  for policy $\pi$</p><script type="math/tex; mode=display">q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s, A_{t}=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s, A_{t}=a\right] \tag{2}\label{2}</script><p>So we have $v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi (a \mid s)q_{\pi}(s,a)$<br>The Bellman equation for the state-value function for policy $\pi$ is</p><script type="math/tex; mode=display">\begin{aligned}v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t | S_t = s]\\& = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\\& =\sum_{a} \pi(a|s) \sum_{s^{\prime}} \sum_{r} p(s^{\prime}, r | s, a) [r + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_t = s^{\prime}]]\\& =\sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r +\gamma v_{\pi}(s^{\prime})]\\\end{aligned} \tag{3}\label{3}</script><p>Similarly, the Bellman equation for the action-value function for policy $\pi$ is</p><script type="math/tex; mode=display">\begin{aligned}q_{\pi}(s,a) &= \mathbb{E}_{\pi}[G_{t} \mid S_{t}=s, A_{t}=a]\\& = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_{t}=a]\\& = \sum_{s^{\prime}} \sum_{r} p(s^{\prime}, r | s, a) [r + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_t = s^{\prime}]]\\& = \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r +\gamma v_{\pi}(s^{\prime})]\\& = \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r +\gamma \sum_{a^{\prime} \in \mathcal{A}} \pi (a^{\prime} \mid s^{\prime}) q_{\pi}(s^{\prime}, a^{\prime})]\\\end{aligned} \tag{4}\label{4}</script><p>The optimal state-value and action-value equations are</p><script type="math/tex; mode=display">v_{*}(s) \doteq \max_{\pi} v_{\pi}(s) \tag{5}\label{5}</script><p>and</p><script type="math/tex; mode=display">q_{*}(s,a) \doteq \max_{\pi} q_{\pi}(s,a) \tag{6}\label{6}</script><p>We have $q_{\ast}(s, a)=\mathbb{E}\left[R_{t+1}+\gamma v_{\ast}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right]$. The Bellman optimality equation for the state-value function is</p><script type="math/tex; mode=display">\begin{aligned}v_{*}(s) &=\max _{a \in \mathcal{A}(s)} q_{\pi_{*}}(s, a) \\&=\max _{a} \mathbb{E}_{\pi_{*}}\left[G_{t} \mid S_{t}=s, A_{t}=a\right] \\&=\max _{a} \mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s, A_{t}=a\right] \\&=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{*}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\&=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{*}\left(s^{\prime}\right)\right]\end{aligned} \tag{7}\label{7}</script><p>and the Bellman optimality equation for the action-value function is</p><script type="math/tex; mode=display">\begin{aligned}q_{*}(s, a) &=\mathbb{E}\left[R_{t+1}+\gamma \max _{a^{\prime}} q_{*}\left(S_{t+1}, a^{\prime}\right) \mid S_{t}=s, A_{t}=a\right] \\&=\sum p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \max_{a^{\prime}}q_{*}\left(s^{\prime}, a^{\prime}\right)\right]\end{aligned} \tag{8}\label{8}</script><p>Let $Q, Q_{t}$ and $V, V_{t}$ be the estimates of $q_{\pi}$ (or $q_{\ast}$) and $v_{\pi}$ (or $v_{\ast}$). The Q-learning (<a href="https://scholar.google.com/scholar?q=Q-learning+(Watkins,+1989">Watkins, 1989</a>&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart)) was developed to directly approximate $q_{\ast}$:</p><script type="math/tex; mode=display">Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right]</script><p>Q-Learning is an off-policy time differential (TD) method. The only and key assumption for the convergence of Q-Learning is that all pairs continue to be updated. “Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, Q has been shown to converge with probability 1 to $q_{\ast}$” (<a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Sutton and Barto</a>).</p><p>For tabular Q-Learning, actually for all tabular RL methods, it is difficult to deal with problems with large state spaces. First, large state space requires large memory to store the tabular action/state values. Second, the time and data should fill the table accurately. If every state encountered will never have been seen before, it is hard to accurately update the table. Therefore, function approximation (supervised learning) is introduced to generalize from previous encounters with different states that are in some sense similar to the current one. Among various function approximation methods, deep neural networks have been recieving increasing attention due to its capability of approximating almost every function that we can imagine.</p><p><a href="https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf">Mnih et al. (2015)</a> proposed the Deep Q-Networks (DQN) algorithm at Nature. This will be used as the baseline of this section. A convolutional neural network is used to learn the Q values using experience replay. The pseudocode of this algorithm is</p><p><div align="center">  <img src="/2021/01/04/A-summary-of-RL-algorithms-1-Deep-Q-Learning/DQN.png" width="55%" height="50%"></div><br>I implemented this original DQN algorithm <a href="https://github.com/Guoqq17/A-summary-of-RL-algorithms.git">here</a>. It is noted that Q-Networks with and without CNN are built.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;1. Introduction&lt;/h1&gt;&lt;p&gt;In this series of blogs, I will summarize and implement some key algorithms in RL. This serves </summary>
      
    
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/categories/Reinforcement-Learning/"/>
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/tags/Reinforcement-Learning/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Algorithms" scheme="http://example.com/tags/Algorithms/"/>
    
  </entry>
  
  <entry>
    <title>4WD Car SLAM and Control (1) Introduction and prototype building</title>
    <link href="http://example.com/2021/01/02/4WD-Car-SLAM-and-Control-1-Introduction-and-prototype-building/"/>
    <id>http://example.com/2021/01/02/4WD-Car-SLAM-and-Control-1-Introduction-and-prototype-building/</id>
    <published>2021-01-02T20:31:29.000Z</published>
    <updated>2021-01-10T19:45:10.547Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction">1. Introduction</h1><p>I decided to start a robotics project since I have been becoming more and more interested in robotics (especially, connected and autonomous vehicles). In this project, I will build a 4WD car prototype using a commercial car kit. After setting the environment of both hardware and software, I will focus on three interesting topics: learn the robot operating system (ROS), simultaneous localization and mapping (SLAM) and vechile control. I currently do not know how far I can go, but we will see.</p><h1 id="Prototype-building-Hardware">2. Prototype building: Hardware</h1><p>Since the focuses (i.e., ROS, SLAM, and control) of this project are mainly software/algorithm oriented, I choose to use a car kit instead of building a self-designed car from scratch. The hardware that I use is <a href="https://www.amazon.com/gp/product/B07GJB7SWZ/ref=ppx_yo_dt_b_asin_title_o00_s01?ie=UTF8&amp;psc=1">Yahboom Super Starter AI Robot Kit</a>, a 4WD electrical car kit coming along with multiple sensors including camera, ultrasonic, and line tracking module. <a href="https://www.amazon.com/gp/product/B07TC2BK1X/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1">Raspberry Pi 4B (64Bit, 4GB)</a> is selected as the onboard computer due to its powerful computation capability. It takes me and Xiaoping (thanks to my wife) 3 hours to assemble the car. It looks like this:</p><div align="center">  <img src="/2021/01/02/4WD-Car-SLAM-and-Control-1-Introduction-and-prototype-building/Yahboom_car_kit_my.jpg" width="30%" height="30%"></div><h1 id="Prototype-building-Development-environment">3. Prototype building: Development environment</h1><p>There are several steps to building the development environment after assembling the hardware, which are shown in the following.</p><h2 id="Install-Ubuntu-Mate-on-Raspberry-Pi">3.1. Install Ubuntu Mate on Raspberry Pi</h2><ul><li>Download <a href="https://ubuntu-mate.org/">Ubuntu Mate</a>.</li><li>Decompress it.</li><li>Use <a href="https://www.balena.io/etcher/">Balena etcher</a> to flash it to a SD card (at least 8GB).</li><li>Inserted the SD card to Raspberry Pi.</li><li>Connect the Raspberry Pi to a monitor and plug in a mouse and a keyboard.</li><li>Power on Raspberry Pi and follow the straightforward installation process.</li><li>After installation, update Ubuntu</li></ul><h2 id="Connect-the-master-laptop-my-Macbook-Pro-and-the-Raspberry-Pi">3.2. Connect the master laptop (my Macbook Pro) and the Raspberry Pi</h2><h3 id="ssh-method">3.2.1. ssh method</h3><ul><li>Install ssh service <code>sudo apt install openssh-server</code></li><li>Enable ssh service of the Raspberry Pi by <code>sudo raspi-config</code> -&gt; <code>ssh</code></li><li>Remove ssh <code>sudo apt autoremove openssh-server</code></li><li>Re-install ssh <code>sudo apt update</code> -&gt; <code>sudo apt install openssh-server</code></li><li>Activate ssh and set autologin <code>sudo service ssh start</code> -&gt; <code>update-rc.d ssh enable</code></li><li>Get the IP of Raspberry Pi <code>ifconfig</code></li><li>Connect Raspberry Pi by ssh <code>ssh &lt;username&gt;@&lt;IP&gt;</code> <!--ssh qiangqiang@192.168.0.188--></li></ul><h3 id="VNC-method">3.2.2. VNC method</h3><ul><li>Install VNC server <code>sudo apt-get install tightvncserver</code></li><li>Activate VNC <code>tightvncserver</code></li><li>Install <a href="https://www.realvnc.com/en/connect/download/viewer/">VNC viewer</a> at Mac and connect to Raspberry Pi <!--192.168.0.188:1, where 1 is the desk number shown after tightvncserver --></li></ul><h2 id="Install-ROS-on-Raspberry-Pi-and-my-laptop">3.3. Install ROS on Raspberry Pi and my laptop</h2><ul><li>Follow the <a href="http://wiki.ros.org/noetic/Installation/Ubuntu">official guide</a> (I used the <a href="https://www.virtualbox.org/">VirtualBox</a> to build a virtual Linux environment on my Mac)</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;1. Introduction&lt;/h1&gt;&lt;p&gt;I decided to start a robotics project since I have been becoming more and more interested in ro</summary>
      
    
    
    
    <category term="Robotics" scheme="http://example.com/categories/Robotics/"/>
    
    
    <category term="4WD car" scheme="http://example.com/tags/4WD-car/"/>
    
    <category term="control" scheme="http://example.com/tags/control/"/>
    
    <category term="slam" scheme="http://example.com/tags/slam/"/>
    
  </entry>
  
</feed>
