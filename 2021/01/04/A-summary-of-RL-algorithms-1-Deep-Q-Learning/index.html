<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1. IntroductionIn this series of blogs, I will summarize and implement some key algorithms in RL. This serves as a self-learning notes on my way to RL. 2. Deep Q-LearningThis section follows the flow">
<meta property="og:type" content="article">
<meta property="og:title" content="A summary of RL algorithms (1) Deep Q-Learning">
<meta property="og:url" content="http://example.com/2021/01/04/A-summary-of-RL-algorithms-1-Deep-Q-Learning/index.html">
<meta property="og:site_name" content="Qiangqiang">
<meta property="og:description" content="1. IntroductionIn this series of blogs, I will summarize and implement some key algorithms in RL. This serves as a self-learning notes on my way to RL. 2. Deep Q-LearningThis section follows the flow">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/01/04/A-summary-of-RL-algorithms-1-Deep-Q-Learning/DQN.png">
<meta property="article:published_time" content="2021-01-04T20:58:12.000Z">
<meta property="article:modified_time" content="2021-01-10T19:44:46.731Z">
<meta property="article:author" content="Qiangqiang Guo">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Tensorflow">
<meta property="article:tag" content="Algorithms">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/01/04/A-summary-of-RL-algorithms-1-Deep-Q-Learning/DQN.png">

<link rel="canonical" href="http://example.com/2021/01/04/A-summary-of-RL-algorithms-1-Deep-Q-Learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>A summary of RL algorithms (1) Deep Q-Learning | Qiangqiang</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Qiangqiang" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Qiangqiang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">A graduate student who likes life and technologies</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/Guoqq17/Guoqq17.github.io" class="github-corner" title="Qiangqiang&#39;s GitHub" aria-label="Qiangqiang&#39;s GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/04/A-summary-of-RL-algorithms-1-Deep-Q-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Qiangqiang Guo">
      <meta itemprop="description" content="The road to all my interests">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qiangqiang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A summary of RL algorithms (1) Deep Q-Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-04 12:58:12" itemprop="dateCreated datePublished" datetime="2021-01-04T12:58:12-08:00">2021-01-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-10 11:44:46" itemprop="dateModified" datetime="2021-01-10T11:44:46-08:00">2021-01-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Introduction">1. Introduction</h1><p>In this series of blogs, I will summarize and implement some key algorithms in RL. This serves as a self-learning notes on my way to RL.</p>
<h1 id="Deep-Q-Learning">2. Deep Q-Learning</h1><p>This section follows the flow of this paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.02298.pdf">Rainbow: Combining Improvements in Deep Reinforcement Learning (Hessel et al., 2017)</a>, which gives a good summary of Deep Q-learning algorithms as well as test different combinations of those algorithms.</p>
<h2 id="Original-deep-Q-Networks-DQN">2.1. Original deep Q-Networks (DQN)</h2><p>Let’s start from the very beginning. Please refer to <a target="_blank" rel="noopener" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Sutton and Barto</a> for details of the definations of a Markov decision process (MDP). The notations used here are same as those used in this book. Define the state-value function for policy $\pi$</p>
<script type="math/tex; mode=display">
v_{\pi}(s) \doteq \mathrm{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]=\mathbf{E}_{\pi}\left[\sum^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right], \text { for all } s \in \mathcal{S} \tag{1} \label{1}</script><!-- reference $\eqref{1}$ -->
<p>and the action-value function  for policy $\pi$</p>
<script type="math/tex; mode=display">
q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s, A_{t}=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s, A_{t}=a\right] \tag{2}\label{2}</script><p>So we have $v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi (a \mid s)q_{\pi}(s,a)$<br>The Bellman equation for the state-value function for policy $\pi$ is</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t | S_t = s]\\
& = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
& =\sum_{a} \pi(a|s) \sum_{s^{\prime}} \sum_{r} p(s^{\prime}, r | s, a) [r + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_t = s^{\prime}]]\\
& =\sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r +\gamma v_{\pi}(s^{\prime})]\\
\end{aligned} \tag{3}\label{3}</script><p>Similarly, the Bellman equation for the action-value function for policy $\pi$ is</p>
<script type="math/tex; mode=display">
\begin{aligned}
q_{\pi}(s,a) &= \mathbb{E}_{\pi}[G_{t} \mid S_{t}=s, A_{t}=a]\\
& = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_{t}=a]\\
& = \sum_{s^{\prime}} \sum_{r} p(s^{\prime}, r | s, a) [r + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_t = s^{\prime}]]\\
& = \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r +\gamma v_{\pi}(s^{\prime})]\\
& = \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r +\gamma \sum_{a^{\prime} \in \mathcal{A}} \pi (a^{\prime} \mid s^{\prime}) q_{\pi}(s^{\prime}, a^{\prime})]\\
\end{aligned} \tag{4}\label{4}</script><p>The optimal state-value and action-value equations are</p>
<script type="math/tex; mode=display">
v_{*}(s) \doteq \max_{\pi} v_{\pi}(s) \tag{5}\label{5}</script><p>and</p>
<script type="math/tex; mode=display">
q_{*}(s,a) \doteq \max_{\pi} q_{\pi}(s,a) \tag{6}\label{6}</script><p>We have $q_{\ast}(s, a)=\mathbb{E}\left[R_{t+1}+\gamma v_{\ast}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right]$. The Bellman optimality equation for the state-value function is</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{*}(s) &=\max _{a \in \mathcal{A}(s)} q_{\pi_{*}}(s, a) \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[G_{t} \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{*}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{*}\left(s^{\prime}\right)\right]
\end{aligned} \tag{7}\label{7}</script><p>and the Bellman optimality equation for the action-value function is</p>
<script type="math/tex; mode=display">
\begin{aligned}
q_{*}(s, a) &=\mathbb{E}\left[R_{t+1}+\gamma \max _{a^{\prime}} q_{*}\left(S_{t+1}, a^{\prime}\right) \mid S_{t}=s, A_{t}=a\right] \\
&=\sum p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \max_{a^{\prime}}q_{*}\left(s^{\prime}, a^{\prime}\right)\right]
\end{aligned} \tag{8}\label{8}</script><p>Let $Q, Q_{t}$ and $V, V_{t}$ be the estimates of $q_{\pi}$ (or $q_{\ast}$) and $v_{\pi}$ (or $v_{\ast}$). The Q-learning (<a target="_blank" rel="noopener" href="https://scholar.google.com/scholar?q=Q-learning+(Watkins,+1989">Watkins, 1989</a>&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart)) was developed to directly approximate $q_{\ast}$:</p>
<script type="math/tex; mode=display">
Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right]</script><p>Q-Learning is an off-policy time differential (TD) method. The only and key assumption for the convergence of Q-Learning is that all pairs continue to be updated. “Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, Q has been shown to converge with probability 1 to $q_{\ast}$” (<a target="_blank" rel="noopener" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Sutton and Barto</a>).</p>
<p>For tabular Q-Learning, actually for all tabular RL methods, it is difficult to deal with problems with large state spaces. First, large state space requires large memory to store the tabular action/state values. Second, the time and data should fill the table accurately. If every state encountered will never have been seen before, it is hard to accurately update the table. Therefore, function approximation (supervised learning) is introduced to generalize from previous encounters with different states that are in some sense similar to the current one. Among various function approximation methods, deep neural networks have been recieving increasing attention due to its capability of approximating almost every function that we can imagine.</p>
<p><a target="_blank" rel="noopener" href="https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf">Mnih et al. (2015)</a> proposed the Deep Q-Networks (DQN) algorithm at Nature. This will be used as the baseline of this section. A convolutional neural network is used to learn the Q values using experience replay. The pseudocode of this algorithm is</p>
<p><div align="center">
  <img src="/2021/01/04/A-summary-of-RL-algorithms-1-Deep-Q-Learning/DQN.png" width="55%" height="50%">
</div><br>I implemented this original DQN algorithm <a target="_blank" rel="noopener" href="https://github.com/Guoqq17/A-summary-of-RL-algorithms.git">here</a>. It is noted that Q-Networks with and without CNN are built.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/Tensorflow/" rel="tag"># Tensorflow</a>
              <a href="/tags/Algorithms/" rel="tag"># Algorithms</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/01/02/4WD-Car-SLAM-and-Control-1-Introduction-and-prototype-building/" rel="prev" title="4WD Car SLAM and Control (1) Introduction and prototype building">
      <i class="fa fa-chevron-left"></i> 4WD Car SLAM and Control (1) Introduction and prototype building
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/01/10/4WD-Car-SLAM-and-Control-2-Initial-testing/" rel="next" title="4WD Car SLAM and Control (2) Initial-testing">
      4WD Car SLAM and Control (2) Initial-testing <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Q-Learning"><span class="nav-text">2. Deep Q-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Original-deep-Q-Networks-DQN"><span class="nav-text">2.1. Original deep Q-Networks (DQN)</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Qiangqiang Guo</p>
  <div class="site-description" itemprop="description">The road to all my interests</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qiangqiang Guo</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
